[
  {
    "objectID": "hw9.html",
    "href": "hw9.html",
    "title": "hw9",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.5     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\", local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike_data\n\n# A tibble: 8,760 × 14\n   Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n   &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 01/12/2017                 254     0              -5.2            37\n 2 01/12/2017                 204     1              -5.5            38\n 3 01/12/2017                 173     2              -6              39\n 4 01/12/2017                 107     3              -6.2            40\n 5 01/12/2017                  78     4              -6              36\n 6 01/12/2017                 100     5              -6.4            37\n 7 01/12/2017                 181     6              -6.6            35\n 8 01/12/2017                 460     7              -7.4            38\n 9 01/12/2017                 930     8              -7.6            37\n10 01/12/2017                 490     9              -6.5            27\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;\n\n\n\n# check for missingness (result: no missing)\ndata1 &lt;- bike_data |&gt; is.na()|&gt; colSums()\ndata1\n\n                     Date         Rented Bike Count                      Hour \n                        0                         0                         0 \n          Temperature(°C)               Humidity(%)          Wind speed (m/s) \n                        0                         0                         0 \n         Visibility (10m) Dew point temperature(°C)   Solar Radiation (MJ/m2) \n                        0                         0                         0 \n             Rainfall(mm)             Snowfall (cm)                   Seasons \n                        0                         0                         0 \n                  Holiday           Functioning Day \n                        0                         0 \n\n\n\n# check column types and values within the columns. All columns seem reasonable except for the Date column. \nattributes(bike_data)$spec\n\ncols(\n  Date = col_character(),\n  `Rented Bike Count` = col_double(),\n  Hour = col_double(),\n  `Temperature(°C)` = col_double(),\n  `Humidity(%)` = col_double(),\n  `Wind speed (m/s)` = col_double(),\n  `Visibility (10m)` = col_double(),\n  `Dew point temperature(°C)` = col_double(),\n  `Solar Radiation (MJ/m2)` = col_double(),\n  `Rainfall(mm)` = col_double(),\n  `Snowfall (cm)` = col_double(),\n  Seasons = col_character(),\n  Holiday = col_character(),\n  `Functioning Day` = col_character()\n)\n\n\n\n# Convert the Date column into an actual date (if need be). Recall the lubridate package\nbike_data &lt;- bike_data |&gt;\nmutate(date = lubridate::dmy(Date)) |&gt;\nselect(-Date)\nbike_data\n\n# A tibble: 8,760 × 14\n   `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)` `Wind speed (m/s)`\n                 &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n 1                 254     0              -5.2            37                2.2\n 2                 204     1              -5.5            38                0.8\n 3                 173     2              -6              39                1  \n 4                 107     3              -6.2            40                0.9\n 5                  78     4              -6              36                2.3\n 6                 100     5              -6.4            37                1.5\n 7                 181     6              -6.6            35                1.3\n 8                 460     7              -7.4            38                0.9\n 9                 930     8              -7.6            37                1.1\n10                 490     9              -6.5            27                0.5\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;, date &lt;date&gt;\n\n\n\n# Now briefly summarize each column to see if there are any weird values.\nsummary(bike_data)\n\n Rented Bike Count      Hour       Temperature(°C)   Humidity(%)   \n Min.   :   0.0    Min.   : 0.00   Min.   :-17.80   Min.   : 0.00  \n 1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50   1st Qu.:42.00  \n Median : 504.5    Median :11.50   Median : 13.70   Median :57.00  \n Mean   : 704.6    Mean   :11.50   Mean   : 12.88   Mean   :58.23  \n 3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50   3rd Qu.:74.00  \n Max.   :3556.0    Max.   :23.00   Max.   : 39.40   Max.   :98.00  \n Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :1.500    Median :1698     Median :  5.100          \n Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :7.400    Max.   :2000     Max.   : 27.200          \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n   Holiday          Functioning Day         date           \n Length:8760        Length:8760        Min.   :2017-12-01  \n Class :character   Class :character   1st Qu.:2018-03-02  \n Mode  :character   Mode  :character   Median :2018-06-01  \n                                       Mean   :2018-06-01  \n                                       3rd Qu.:2018-08-31  \n                                       Max.   :2018-11-30  \n\n# Check the character columns\nbike_data$Seasons |&gt; unique()\n\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n\n\n# Turn the character variables (Seasons, Holiday, and Functioning Day) into factors.\nbike_data &lt;- bike_data |&gt;\nmutate(seasons = factor(Seasons), holiday = factor(Holiday), fn_day = factor(`Functioning Day`)) |&gt;\nselect(-Seasons, -Holiday, -`Functioning Day`)\nbike_data\n\n# A tibble: 8,760 × 14\n   `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)` `Wind speed (m/s)`\n                 &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n 1                 254     0              -5.2            37                2.2\n 2                 204     1              -5.5            38                0.8\n 3                 173     2              -6              39                1  \n 4                 107     3              -6.2            40                0.9\n 5                  78     4              -6              36                2.3\n 6                 100     5              -6.4            37                1.5\n 7                 181     6              -6.6            35                1.3\n 8                 460     7              -7.4            38                0.9\n 9                 930     8              -7.6            37                1.1\n10                 490     9              -6.5            27                0.5\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, date &lt;date&gt;, seasons &lt;fct&gt;,\n#   holiday &lt;fct&gt;, fn_day &lt;fct&gt;\n\n\n\n# Lastly, rename the all the variables to have easy to use names (I use lower snake case but whatever you’d like is fine\nbike_data &lt;- bike_data |&gt;\nrename('bike_count' = `Rented Bike Count`,\n'hour' = \"Hour\",\n\"temp\" = `Temperature(°C)`,\n\"wind_speed\" = `Wind speed (m/s)`,\n\"humidity\" = `Humidity(%)`,\n\"vis\" = `Visibility (10m)`,\n\"dew_point_temp\" = `Dew point temperature(°C)`,\n\"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n\"rainfall\" = \"Rainfall(mm)\",\n\"snowfall\" = `Snowfall (cm)`)\nbike_data\n\n# A tibble: 8,760 × 14\n   bike_count  hour  temp humidity wind_speed   vis dew_point_temp\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1        254     0  -5.2       37        2.2  2000          -17.6\n 2        204     1  -5.5       38        0.8  2000          -17.6\n 3        173     2  -6         39        1    2000          -17.7\n 4        107     3  -6.2       40        0.9  2000          -17.6\n 5         78     4  -6         36        2.3  2000          -18.6\n 6        100     5  -6.4       37        1.5  2000          -18.7\n 7        181     6  -6.6       35        1.3  2000          -19.5\n 8        460     7  -7.4       38        0.9  2000          -19.3\n 9        930     8  -7.6       37        1.1  2000          -19.8\n10        490     9  -6.5       27        0.5  1928          -22.4\n# ℹ 8,750 more rows\n# ℹ 7 more variables: solar_radiation &lt;dbl&gt;, rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;,\n#   date &lt;date&gt;, seasons &lt;fct&gt;, holiday &lt;fct&gt;, fn_day &lt;fct&gt;\n\n\n\n# fn_day variable implies they were out of commission sometimes. Let’s remove those observations and that variable\nbike_data &lt;- bike_data |&gt;\nfilter(fn_day == \"Yes\") |&gt;\nselect(-fn_day)\nbike_data\n\n# A tibble: 8,465 × 13\n   bike_count  hour  temp humidity wind_speed   vis dew_point_temp\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1        254     0  -5.2       37        2.2  2000          -17.6\n 2        204     1  -5.5       38        0.8  2000          -17.6\n 3        173     2  -6         39        1    2000          -17.7\n 4        107     3  -6.2       40        0.9  2000          -17.6\n 5         78     4  -6         36        2.3  2000          -18.6\n 6        100     5  -6.4       37        1.5  2000          -18.7\n 7        181     6  -6.6       35        1.3  2000          -19.5\n 8        460     7  -7.4       38        0.9  2000          -19.3\n 9        930     8  -7.6       37        1.1  2000          -19.8\n10        490     9  -6.5       27        0.5  1928          -22.4\n# ℹ 8,455 more rows\n# ℹ 6 more variables: solar_radiation &lt;dbl&gt;, rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;,\n#   date &lt;date&gt;, seasons &lt;fct&gt;, holiday &lt;fct&gt;\n\n\n\n# To simplify our analysis, we’ll summarize across the hours so that each day has one observation associated with it. Let’s group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.\nbike_data &lt;- bike_data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n      temp = mean(temp),\n      humidity = mean(humidity),\n      wind_speed = mean(wind_speed),\n      vis = mean(vis),\n      dew_point_temp = mean(dew_point_temp),\n      solar_radiation = mean(solar_radiation),\n      rainfall = sum(rainfall),\n      snowfall = sum(snowfall)) |&gt;\nungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\nbike_data\n\n# A tibble: 353 × 12\n   date       seasons holiday    bike_count    temp humidity wind_speed   vis\n   &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 2017-12-01 Winter  No Holiday       9539 -2.45       45.9      1.54  1871.\n 2 2017-12-02 Winter  No Holiday       8523  1.32       62.0      1.71  1471.\n 3 2017-12-03 Winter  No Holiday       7222  4.88       81.5      1.61   456.\n 4 2017-12-04 Winter  No Holiday       8729 -0.304      52.5      3.45  1363.\n 5 2017-12-05 Winter  No Holiday       8307 -4.46       36.4      1.11  1959.\n 6 2017-12-06 Winter  No Holiday       6669  0.0458     70.8      0.696 1187.\n 7 2017-12-07 Winter  No Holiday       8549  1.09       67.5      1.69   949.\n 8 2017-12-08 Winter  No Holiday       8032 -3.82       41.8      1.85  1872.\n 9 2017-12-09 Winter  No Holiday       7233 -0.846      46        1.08  1861.\n10 2017-12-10 Winter  No Holiday       3453  1.19       69.7      2.00  1043.\n# ℹ 343 more rows\n# ℹ 4 more variables: dew_point_temp &lt;dbl&gt;, solar_radiation &lt;dbl&gt;,\n#   rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;\n\n\n\n# Recreate your basic summary stats and then create some plots to explore relationships. Report correlation between your numeric variables as well\n# Looks to be right skewed with a pretty large standard deviation.\nbike_data |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 1 × 6\n  bike_count_mean bike_count_median bike_count_sd bike_count_IQR bike_count_min\n            &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1          17485.             18563         9937.          19318            977\n# ℹ 1 more variable: bike_count_max &lt;dbl&gt;\n\n\n\nbike_data |&gt;\ngroup_by(holiday) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 2 × 7\n  holiday    bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;                &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Holiday             12700.             7184         10504.         16576 \n2 No Holiday          17727.            19104.         9862.         19168.\n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\n\nbike_data |&gt;\ngroup_by(seasons) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 4 × 7\n  seasons bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Autumn           22099.            23350          6711.         10733 \n2 Spring           17910.            17590          8357.         14362.\n3 Summer           24818.            25572.         7297.          9308.\n4 Winter            5413.             5498          1808.          2634.\n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\n\n# Differences are pretty big in the Winter and Autumn but not the other seasons. Perhaps an interaction between these two variables is important.\nbike_data |&gt;\ngroup_by(seasons, holiday) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n`summarise()` has grouped output by 'seasons'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 8\n# Groups:   seasons [4]\n  seasons holiday bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Autumn  Holiday          22754.            21705          5642.          5740 \n2 Autumn  No Hol…          22065.            23472          6792.         10734 \n3 Spring  Holiday          15247.            13790         10917.         10844 \n4 Spring  No Hol…          18002.            17730          8322.         14224.\n5 Summer  Holiday          24532.            24532.         8438.          5966.\n6 Summer  No Hol…          24824.            25572.         7324.          9165 \n7 Winter  Holiday           3759              3454.         1561.          1060.\n8 Winter  No Hol…           5574.             5609          1757.          2564 \n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\n\n# do some correlation\n# Definitely a few moderate relationships with bike_count here (temp and solar_radiation). temp and dew_point_temp are obviously pretty related. humidity and vis along with humidity and dew_point_temp as well.\nbike_data |&gt;\nselect(where(is.numeric)) |&gt;\ncor() |&gt;\nround(3)\n\n                bike_count   temp humidity wind_speed    vis dew_point_temp\nbike_count           1.000  0.753    0.036     -0.193  0.166          0.650\ntemp                 0.753  1.000    0.404     -0.261  0.002          0.963\nhumidity             0.036  0.404    1.000     -0.234 -0.559          0.632\nwind_speed          -0.193 -0.261   -0.234      1.000  0.206         -0.288\nvis                  0.166  0.002   -0.559      0.206  1.000         -0.154\ndew_point_temp       0.650  0.963    0.632     -0.288 -0.154          1.000\nsolar_radiation      0.736  0.550   -0.274      0.096  0.271          0.383\nrainfall            -0.239  0.145    0.529     -0.102 -0.222          0.265\nsnowfall            -0.265 -0.267    0.065      0.021 -0.102         -0.210\n                solar_radiation rainfall snowfall\nbike_count                0.736   -0.239   -0.265\ntemp                      0.550    0.145   -0.267\nhumidity                 -0.274    0.529    0.065\nwind_speed                0.096   -0.102    0.021\nvis                       0.271   -0.222   -0.102\ndew_point_temp            0.383    0.265   -0.210\nsolar_radiation           1.000   -0.323   -0.233\nrainfall                 -0.323    1.000   -0.023\nsnowfall                 -0.233   -0.023    1.000\n\n\n\n# do some visualizations. Some expected trends here and we can see that once it gets pretty hot, bike rentals slow.\nggplot(bike_data, aes(x = temp, y = bike_count)) +\ngeom_jitter(aes(color = seasons)) +\nfacet_grid(~holiday)\n\n\n\n\n\n# More solar radiation is associated with more bike rentals\nggplot(bike_data, aes(x = solar_radiation, y = bike_count)) +\ngeom_point(aes(color = seasons)) +\nfacet_grid(~holiday)\n\n\n\n\n\n# Use functions from tidymodels to split the data into a training and test set (75/25 split). Use the strata argument to stratify the split on the seasons variable. On the training set, create a 10 fold CV split\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\nbike_10_fold\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [236/27]&gt; Fold01\n 2 &lt;split [236/27]&gt; Fold02\n 3 &lt;split [236/27]&gt; Fold03\n 4 &lt;split [237/26]&gt; Fold04\n 5 &lt;split [237/26]&gt; Fold05\n 6 &lt;split [237/26]&gt; Fold06\n 7 &lt;split [237/26]&gt; Fold07\n 8 &lt;split [237/26]&gt; Fold08\n 9 &lt;split [237/26]&gt; Fold09\n10 &lt;split [237/26]&gt; Fold10\n\n\n\n# create some recipes\n\nMLR_rec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)  \nMLR_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\"))\n\n\n• Variables removed: date and date_dow\n\n\n• Dummy variables from: seasons, holiday, day_type\n\n\n• Centering and scaling for: all_numeric() and -bike_count\n\n\n\n# For the 2nd recipe\nMLR_rec2 &lt;- MLR_rec1 |&gt;\nstep_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\nstarts_with(\"seasons\")*temp +\ntemp*rainfall)\nMLR_rec2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\"))\n\n\n• Variables removed: date and date_dow\n\n\n• Dummy variables from: seasons, holiday, day_type\n\n\n• Centering and scaling for: all_numeric() and -bike_count\n\n\n• Interactions with: ...\n\n\n\n# For the 3rd recipe\nMLR_rec3 &lt;- MLR_rec2 |&gt;\n  step_poly(temp,\n    wind_speed,\n    vis,\n    dew_point_temp,\n    solar_radiation,\n    rainfall,\n    snowfall,\n    degree = 2)\n\n\n# Now we can set up our linear model fit\nMLR_spec &lt;- linear_reg() |&gt;\nset_engine(\"lm\")\n\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec1) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n\nMLR_CV_fit2 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec2) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n\nMLR_CV_fit3 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec3) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n  \n    \nMLR_CV_fit1\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\n# Get our metrics. The last model appears to be the best\nrbind(MLR_CV_fit1 |&gt; collect_metrics(),\nMLR_CV_fit2 |&gt; collect_metrics(),\nMLR_CV_fit3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4284.       10 165.     Preprocessor1_Model1\n2 rsq     standard      0.822    10   0.0151 Preprocessor1_Model1\n3 rmse    standard   3156.       10 267.     Preprocessor1_Model1\n4 rsq     standard      0.898    10   0.0176 Preprocessor1_Model1\n5 rmse    standard   3070.       10 213.     Preprocessor1_Model1\n6 rsq     standard      0.903    10   0.0142 Preprocessor1_Model1\n\n\n\n# fit that to the entire training set and then see how it performs on the test set.\nfinal_fit &lt;- workflow() |&gt;\nadd_recipe(MLR_rec1) |&gt;\nadd_model(MLR_spec) |&gt;\nlast_fit(bike_split)\n\nfinal_fit |&gt;\ncollect_metrics() |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3980.    Preprocessor1_Model1\n2 rsq     standard       0.846 Preprocessor1_Model1\n\n\n\n# Obtain the final model (fit on the entire training set) coefficient table using tidy().\nfinal_fit |&gt;\nextract_fit_parsnip() |&gt;\ntidy()|&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# A tibble: 14 × 5\n   term               estimate std.error statistic p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)         17446.       252.    69.3     0    \n 2 temp                -2439.      5215.    -0.468   0.64 \n 3 humidity            -1927.      1904.    -1.01    0.313\n 4 wind_speed           -523.       286.    -1.83    0.069\n 5 vis                   -63.7      361.    -0.177   0.86 \n 6 dew_point_temp       7143.      6143.     1.16    0.246\n 7 solar_radiation      4088.       473.     8.64    0    \n 8 rainfall            -1779.       333.    -5.35    0    \n 9 snowfall             -317.       276.    -1.15    0.25 \n10 seasons_Spring      -2528.       355.    -7.12    0    \n11 seasons_Summer      -1670.       442.    -3.78    0    \n12 seasons_Winter      -3684.       501.    -7.35    0    \n13 holiday_No.Holiday    835.       256.     3.26    0.001\n14 day_type_Weekend    -1050.       256.    -4.10    0    \n\n\n\n# My homework 9 starts here...\n# define LASSO model specification\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n# define recipe. Use the same preprocessing steps as the previous MLR model (MLR_rec1)\nlasso_rec &lt;- MLR_rec1\n\nlasso_workflow &lt;- workflow() |&gt;\n  add_recipe(lasso_rec) |&gt;\n  add_model(lasso_spec)\n\n# define cross-validation and tuning grid\nset.seed(123)\nbike_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Define a grid of penalty values\npenalty_grid &lt;- grid_regular(penalty(range = c(-3, 1)), levels = 30) # log10 scale\n\n# tuning\nlasso_tune &lt;- lasso_workflow |&gt;\n  tune_grid(\n    resamples = bike_folds,\n    grid = penalty_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nWarning: package 'Matrix' was built under R version 4.3.3\n\n# per Dr. Post, use select_best() and use the smallest penalty\nlasso_best &lt;- lasso_tune |&gt;\n  select_best(metric = \"rmse\")\nlasso_best\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.001 Preprocessor1_Model01\n\nlasso_final_workflow &lt;- lasso_workflow |&gt;\n  finalize_workflow(lasso_best)\n\nlasso_final_fit &lt;- lasso_final_workflow |&gt;\n  last_fit(bike_split)\nlasso_metrics &lt;- lasso_final_fit |&gt;\n  collect_metrics()\n\n# Display metrics. It shows rmse = 3999.44, rsq = 0.844\nlasso_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3999.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n\n# or display them all. However, LASSO does not seem to be a good fit.\nrbind(\n  lasso_tune |&gt; collect_metrics() |&gt; mutate(model = \"LASSO\")\n)\n\n# A tibble: 60 × 8\n   penalty .metric .estimator     mean     n  std_err .config              model\n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;\n 1 0.001   rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 2 0.001   rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 3 0.00137 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 4 0.00137 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 5 0.00189 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 6 0.00189 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 7 0.00259 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 8 0.00259 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 9 0.00356 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n10 0.00356 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n# ℹ 50 more rows\n\n\n\n# add a (tuned) regression Tree model\n# define regression tree model specification\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(), \n  tree_depth = tune(),      \n  min_n = tune()            \n) |&gt; \n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# recipe. Use the same recipe as before (MLR_rec1).\ntree_rec &lt;- MLR_rec1\n# Combine the recipe and the model specification.\ntree_workflow &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_spec)\n\nset.seed(123)\nbike_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Define a tuning grid\ntree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-3, 0)), # Log10 scale\n  tree_depth(range = c(1, 10)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Below: issue with some computations. estimate is constant and has 0 standard deviation (folder might have insufficient variability in 'bike_count')\ntree_tune &lt;- tree_workflow |&gt;\n  tune_grid(\n    resamples = bike_folds,\n    grid = tree_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\ntree_best &lt;- tree_tune |&gt;\n  select_best(metric = \"rmse\")\n\n# Print the best hyperparameters. Return: cost_complexity 0.001, tree_depth 7, and min_n 8.\ntree_best\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1           0.001          7     8 Preprocessor1_Model091\n\n# use the best hyperparameters to finalize workflow\ntree_final_workflow &lt;- tree_workflow |&gt;\n  finalize_workflow(tree_best)\n\n# fit the best regression tree model on entire training data\ntree_final_fit &lt;- tree_final_workflow |&gt;\n  last_fit(bike_split)\n\ntree_metrics &lt;- tree_final_fit |&gt;\n  collect_metrics()\n\n# Display metrics. rmse = 3046.078, rsq = 0.91\ntree_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3046.    Preprocessor1_Model1\n2 rsq     standard       0.911 Preprocessor1_Model1\n\n\n\n# Add a (tuned) bagged tree model\n# Define the Bagged Tree Model Specification. Bagging: creating multiple decision trees and averaging their predictions\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.3.3\n\n# \n# bagged_tree_spec &lt;- bag_tree() |&gt;\n#   set_engine(\"rpart\", times = tune()) |&gt;\n#   set_mode(\"regression\")\n\n\nbagged_tree_spec &lt;- bag_tree(\n  cost_complexity = tune(),  # Complexity parameter for pruning\n  tree_depth = tune(),       # Maximum depth of individual trees\n  min_n = tune()             # Minimum number of observations in a terminal node\n) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# Use the same preprocessing recipe (MLR_rec1)\n\nbagged_tree_rec &lt;- MLR_rec1\n\n# Create the Workflow, Combine the recipe and model specification into a workflow\nbagged_tree_workflow &lt;- workflow() |&gt;\n  add_recipe(bagged_tree_rec) |&gt;\n  add_model(bagged_tree_spec)\n\n# Define Cross-Validation and Tuning Grid\n\nbagged_tree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-3, 0)), # Log10 scale\n  tree_depth(range = c(5, 20)),     # Depth of the tree\n  min_n(range = c(2, 10)),          # Minimum samples per node\n  levels = 5\n)\n\n# # following is commanded out for now.\n# \n# set.seed(123)\n# bagged_tree_folds &lt;- vfold_cv(bike_train, v = 10)\n# \n# # Define the tuning grid\n# bagged_tree_grid &lt;- grid_regular(\n#   times(range = c(10, 50)), # Number of trees to bag\n#   levels = 5\n# )\n# \n# bagged_tree_grid &lt;- tibble(\n#   times = seq(10, 50, by = 10) # Specify the number of trees to bag\n# )\n\n# perform tuning. error\nset.seed(123)\n\nbagged_tree_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# long execution time\nbagged_tree_tune &lt;- bagged_tree_workflow |&gt;\n  tune_grid(\n    resamples = bagged_tree_folds,\n    grid = bagged_tree_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\nbagged_tree_metrics &lt;- bagged_tree_tune |&gt; \n  collect_metrics() |&gt; \n  mutate(model = \"Bagged Tree\")\n\nrbind(bagged_tree_metrics)\n\n# A tibble: 250 × 10\n   cost_complexity tree_depth min_n .metric .estimator     mean     n  std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1         0.001            5     2 rmse    standard   3369.       10 220.    \n 2         0.001            5     2 rsq     standard      0.885    10   0.0144\n 3         0.00562          5     2 rmse    standard   3509.       10 173.    \n 4         0.00562          5     2 rsq     standard      0.877    10   0.0145\n 5         0.0316           5     2 rmse    standard   4278.       10 224.    \n 6         0.0316           5     2 rsq     standard      0.812    10   0.0272\n 7         0.178            5     2 rmse    standard   5773.       10 445.    \n 8         0.178            5     2 rsq     standard      0.661    10   0.0515\n 9         1                5     2 rmse    standard   9878.       10 151.    \n10         1                5     2 rsq     standard    NaN         0  NA     \n# ℹ 240 more rows\n# ℹ 2 more variables: .config &lt;chr&gt;, model &lt;chr&gt;\n\n\n\nrf_spec &lt;- rand_forest(\n  mtry = tune(),   # Number of predictors to randomly select at each split\n  trees = tune(),  # Number of trees in the forest\n  min_n = tune()   # Minimum number of data points in a terminal node\n) |&gt;\n  set_engine(\"randomForest\") |&gt;  # Use the 'randomForest' engine\n  set_mode(\"regression\")\n\n# Use the same preprocessing recipe (MLR_rec1)\nrf_rec &lt;- MLR_rec1\n\n# Combine the recipe and the Random Forest model into a workflow\nrf_workflow &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_spec)\n\n\nset.seed(123)\nrf_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Define a tuning grid for the hyperparameters\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, ncol(bike_train) - 1)), # Number of predictors\n  trees(range = c(50, 500)),               # Number of trees\n  min_n(range = c(5, 20)),                 # Minimum node size\n  levels = 5                               # Levels for each parameter\n)\n\n\n# perform tuning. run the tuning process\nset.seed(123)\nrf_tune &lt;- rf_workflow |&gt;\n  tune_grid(\n    resamples = rf_folds,\n    grid = rf_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\nrf_metrics &lt;- rf_tune |&gt; \n  collect_metrics() |&gt; \n  mutate(model = \"Random Forest\")\n\nrbind(rf_metrics)\n\n# A tibble: 250 × 10\n    mtry trees min_n .metric .estimator     mean     n   std_err .config   model\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n 1     2    50     5 rmse    standard   3453.       10 179.      Preproce… Rand…\n 2     2    50     5 rsq     standard      0.891    10   0.00907 Preproce… Rand…\n 3     4    50     5 rmse    standard   3167.       10 160.      Preproce… Rand…\n 4     4    50     5 rsq     standard      0.903    10   0.00921 Preproce… Rand…\n 5     6    50     5 rmse    standard   3087.       10 185.      Preproce… Rand…\n 6     6    50     5 rsq     standard      0.907    10   0.0119  Preproce… Rand…\n 7     8    50     5 rmse    standard   3054.       10 205.      Preproce… Rand…\n 8     8    50     5 rsq     standard      0.907    10   0.0131  Preproce… Rand…\n 9    11    50     5 rmse    standard   3094.       10 199.      Preproce… Rand…\n10    11    50     5 rsq     standard      0.905    10   0.0121  Preproce… Rand…\n# ℹ 240 more rows\n\n# sort by rmse\nrf_best_rmse &lt;- rf_metrics |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# by r2\nrf_best_rsq &lt;- rf_metrics |&gt;\n  filter(.metric == \"rsq\") |&gt;\n  arrange(desc(mean))\n\n\n# finalize and evaluate it on the test set\nrf_best &lt;- rf_tune |&gt; select_best(metric = \"rmse\")\n\n\nfinal_rf_workflow &lt;- rf_workflow |&gt; \n  finalize_workflow(rf_best)\n\nfinal_rf_fit &lt;- final_rf_workflow |&gt; \n  last_fit(bike_split)\n\n# Collect metrics from the test set\nfinal_rf_metrics &lt;- final_rf_fit |&gt; \n  collect_metrics()\n\nfinal_rf_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2670.    Preprocessor1_Model1\n2 rsq     standard       0.932 Preprocessor1_Model1\n\n\n\nlasso_best &lt;- lasso_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_lasso_workflow &lt;- lasso_workflow |&gt; \n  finalize_workflow(lasso_best)\n\n# Save test set evaluation for LASSO\nlasso_metrics &lt;- final_lasso_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"LASSO\")\n\n\n# regression tree\nreg_tree_best &lt;- tree_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_regression_tree_workflow &lt;- tree_final_workflow |&gt; \n  finalize_workflow(reg_tree_best)\n\nreg_tree_metrics &lt;- tree_final_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt; \n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt; \n  mutate(model = \"Regression Tree\")\n\n# bagged tree\nbagged_tree_best &lt;- bagged_tree_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_bagged_tree_workflow &lt;- bagged_tree_workflow |&gt; \n  finalize_workflow(bagged_tree_best)\n\n# Save test set evaluation for Bagged Tree\nbagged_tree_metrics &lt;- final_bagged_tree_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"Bagged Tree\")\n\n\n# random forest model\nrf_best &lt;- rf_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_rf_workflow &lt;- rf_workflow |&gt; \n  finalize_workflow(rf_best)\n\n# Save test set evaluation for Random Forest\nrf_metrics &lt;- final_rf_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"Random Forest\")\n\n\nall_metrics &lt;- bind_rows(\n  lasso_metrics,\n  reg_tree_metrics,\n  bagged_tree_metrics,\n  rf_metrics\n)\n\nprint(all_metrics)\n\n# A tibble: 4 × 5\n  .metric .estimator .estimate .config              model          \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;          \n1 rmse    standard       3999. Preprocessor1_Model1 LASSO          \n2 rmse    standard       3046. Preprocessor1_Model1 Regression Tree\n3 rmse    standard       2954. Preprocessor1_Model1 Bagged Tree    \n4 rmse    standard       2750. Preprocessor1_Model1 Random Forest  \n\n\n\nlasso_best &lt;- lasso_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_lasso_workflow &lt;- lasso_workflow |&gt; \n  finalize_workflow(lasso_best)\n\nfinal_lasso_fit &lt;- final_lasso_workflow |&gt; \n  fit(data = bike_train)\n\nlasso_model &lt;- final_lasso_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\nprint(lasso_model)\n\n# A tibble: 14 × 3\n   term               estimate penalty\n   &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)          17446.   0.001\n 2 temp                   389.   0.001\n 3 humidity              -887.   0.001\n 4 wind_speed            -522.   0.001\n 5 vis                      0    0.001\n 6 dew_point_temp        3752.   0.001\n 7 solar_radiation       4065.   0.001\n 8 rainfall             -1841.   0.001\n 9 snowfall              -336.   0.001\n10 seasons_Spring       -2505.   0.001\n11 seasons_Summer       -1607.   0.001\n12 seasons_Winter       -3653.   0.001\n13 holiday_No.Holiday     820.   0.001\n14 day_type_Weekend     -1060.   0.001\n\n\n\nfinal_fit &lt;- workflow |&gt; \n  fit(data = bike_train)\n\nmlr_model &lt;- final_mlr_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\n\nprint(mlr_model)\n\n\nreg_tree_model &lt;- tree_final_fit |&gt; \n  extract_fit_engine()\n\nif (!requireNamespace(\"rpart.plot\", quietly = TRUE)) {\n  install.packages(\"rpart.plot\")\n}\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\n# Plot the regression tree\nrpart.plot(reg_tree_model)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\nfinal_bagged_tree_fit &lt;- final_bagged_tree_workflow |&gt; \n  fit(data = bike_train)\n\nfinal_rf_fit &lt;- final_rf_workflow |&gt; \n  fit(data = bike_train)\n\nrf_model &lt;- final_rf_fit |&gt; \n  extract_fit_engine()\n\n# # Extract variable importance\n# rf_importance &lt;- as.data.frame(rf_model$variable.importance) |&gt; \n#   rownames_to_column(var = \"Variable\") |&gt; \n#   rename(Importance = 2) |&gt; \n#   arrange(desc(Importance))\n\nif (!is.null(rf_model$importance)) {\n  rf_importance &lt;- as.data.frame(rf_model$importance) |&gt;\n    tibble::rownames_to_column(var = \"Variable\") |&gt;\n    arrange(desc(IncNodePurity))\n} else {\n  stop(\"Variable importance is not available in the model output.\")\n}\n\nlibrary(ggplot2)\n\nrf_importance &lt;- as.data.frame(rf_model$importance, stringsAsFactors = FALSE) |&gt; \n  tibble::rownames_to_column(var = \"Variable\") |&gt; \n  rename(Importance = IncNodePurity)\n\nprint(rf_importance)\n\n             Variable  Importance\n1                temp 12438261261\n2            humidity   744875642\n3          wind_speed   354338807\n4                 vis   506832939\n5      dew_point_temp  2391134881\n6     solar_radiation  4674690597\n7            rainfall  1060319136\n8            snowfall    20765096\n9      seasons_Spring   256379773\n10     seasons_Summer   115005444\n11     seasons_Winter  2659083462\n12 holiday_No.Holiday    58525762\n13   day_type_Weekend   109391163\n\nggplot(rf_importance, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance - Random Forest\",\n    x = \"Variable\",\n    y = \"Importance\"\n  )"
  },
  {
    "objectID": "hw9.html#read-in-the-data",
    "href": "hw9.html#read-in-the-data",
    "title": "hw9",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.5     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\nWarning: package 'broom' was built under R version 4.3.3\n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.2\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nbike_data &lt;- read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\", local = locale(encoding = \"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nbike_data\n\n# A tibble: 8,760 × 14\n   Date       `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)`\n   &lt;chr&gt;                    &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 01/12/2017                 254     0              -5.2            37\n 2 01/12/2017                 204     1              -5.5            38\n 3 01/12/2017                 173     2              -6              39\n 4 01/12/2017                 107     3              -6.2            40\n 5 01/12/2017                  78     4              -6              36\n 6 01/12/2017                 100     5              -6.4            37\n 7 01/12/2017                 181     6              -6.6            35\n 8 01/12/2017                 460     7              -7.4            38\n 9 01/12/2017                 930     8              -7.6            37\n10 01/12/2017                 490     9              -6.5            27\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Wind speed (m/s)` &lt;dbl&gt;, `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;\n\n\n\n# check for missingness (result: no missing)\ndata1 &lt;- bike_data |&gt; is.na()|&gt; colSums()\ndata1\n\n                     Date         Rented Bike Count                      Hour \n                        0                         0                         0 \n          Temperature(°C)               Humidity(%)          Wind speed (m/s) \n                        0                         0                         0 \n         Visibility (10m) Dew point temperature(°C)   Solar Radiation (MJ/m2) \n                        0                         0                         0 \n             Rainfall(mm)             Snowfall (cm)                   Seasons \n                        0                         0                         0 \n                  Holiday           Functioning Day \n                        0                         0 \n\n\n\n# check column types and values within the columns. All columns seem reasonable except for the Date column. \nattributes(bike_data)$spec\n\ncols(\n  Date = col_character(),\n  `Rented Bike Count` = col_double(),\n  Hour = col_double(),\n  `Temperature(°C)` = col_double(),\n  `Humidity(%)` = col_double(),\n  `Wind speed (m/s)` = col_double(),\n  `Visibility (10m)` = col_double(),\n  `Dew point temperature(°C)` = col_double(),\n  `Solar Radiation (MJ/m2)` = col_double(),\n  `Rainfall(mm)` = col_double(),\n  `Snowfall (cm)` = col_double(),\n  Seasons = col_character(),\n  Holiday = col_character(),\n  `Functioning Day` = col_character()\n)\n\n\n\n# Convert the Date column into an actual date (if need be). Recall the lubridate package\nbike_data &lt;- bike_data |&gt;\nmutate(date = lubridate::dmy(Date)) |&gt;\nselect(-Date)\nbike_data\n\n# A tibble: 8,760 × 14\n   `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)` `Wind speed (m/s)`\n                 &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n 1                 254     0              -5.2            37                2.2\n 2                 204     1              -5.5            38                0.8\n 3                 173     2              -6              39                1  \n 4                 107     3              -6.2            40                0.9\n 5                  78     4              -6              36                2.3\n 6                 100     5              -6.4            37                1.5\n 7                 181     6              -6.6            35                1.3\n 8                 460     7              -7.4            38                0.9\n 9                 930     8              -7.6            37                1.1\n10                 490     9              -6.5            27                0.5\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, Seasons &lt;chr&gt;, Holiday &lt;chr&gt;,\n#   `Functioning Day` &lt;chr&gt;, date &lt;date&gt;\n\n\n\n# Now briefly summarize each column to see if there are any weird values.\nsummary(bike_data)\n\n Rented Bike Count      Hour       Temperature(°C)   Humidity(%)   \n Min.   :   0.0    Min.   : 0.00   Min.   :-17.80   Min.   : 0.00  \n 1st Qu.: 191.0    1st Qu.: 5.75   1st Qu.:  3.50   1st Qu.:42.00  \n Median : 504.5    Median :11.50   Median : 13.70   Median :57.00  \n Mean   : 704.6    Mean   :11.50   Mean   : 12.88   Mean   :58.23  \n 3rd Qu.:1065.2    3rd Qu.:17.25   3rd Qu.: 22.50   3rd Qu.:74.00  \n Max.   :3556.0    Max.   :23.00   Max.   : 39.40   Max.   :98.00  \n Wind speed (m/s) Visibility (10m) Dew point temperature(°C)\n Min.   :0.000    Min.   :  27     Min.   :-30.600          \n 1st Qu.:0.900    1st Qu.: 940     1st Qu.: -4.700          \n Median :1.500    Median :1698     Median :  5.100          \n Mean   :1.725    Mean   :1437     Mean   :  4.074          \n 3rd Qu.:2.300    3rd Qu.:2000     3rd Qu.: 14.800          \n Max.   :7.400    Max.   :2000     Max.   : 27.200          \n Solar Radiation (MJ/m2)  Rainfall(mm)     Snowfall (cm)       Seasons         \n Min.   :0.0000          Min.   : 0.0000   Min.   :0.00000   Length:8760       \n 1st Qu.:0.0000          1st Qu.: 0.0000   1st Qu.:0.00000   Class :character  \n Median :0.0100          Median : 0.0000   Median :0.00000   Mode  :character  \n Mean   :0.5691          Mean   : 0.1487   Mean   :0.07507                     \n 3rd Qu.:0.9300          3rd Qu.: 0.0000   3rd Qu.:0.00000                     \n Max.   :3.5200          Max.   :35.0000   Max.   :8.80000                     \n   Holiday          Functioning Day         date           \n Length:8760        Length:8760        Min.   :2017-12-01  \n Class :character   Class :character   1st Qu.:2018-03-02  \n Mode  :character   Mode  :character   Median :2018-06-01  \n                                       Mean   :2018-06-01  \n                                       3rd Qu.:2018-08-31  \n                                       Max.   :2018-11-30  \n\n# Check the character columns\nbike_data$Seasons |&gt; unique()\n\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n\n\n# Turn the character variables (Seasons, Holiday, and Functioning Day) into factors.\nbike_data &lt;- bike_data |&gt;\nmutate(seasons = factor(Seasons), holiday = factor(Holiday), fn_day = factor(`Functioning Day`)) |&gt;\nselect(-Seasons, -Holiday, -`Functioning Day`)\nbike_data\n\n# A tibble: 8,760 × 14\n   `Rented Bike Count`  Hour `Temperature(°C)` `Humidity(%)` `Wind speed (m/s)`\n                 &lt;dbl&gt; &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;              &lt;dbl&gt;\n 1                 254     0              -5.2            37                2.2\n 2                 204     1              -5.5            38                0.8\n 3                 173     2              -6              39                1  \n 4                 107     3              -6.2            40                0.9\n 5                  78     4              -6              36                2.3\n 6                 100     5              -6.4            37                1.5\n 7                 181     6              -6.6            35                1.3\n 8                 460     7              -7.4            38                0.9\n 9                 930     8              -7.6            37                1.1\n10                 490     9              -6.5            27                0.5\n# ℹ 8,750 more rows\n# ℹ 9 more variables: `Visibility (10m)` &lt;dbl&gt;,\n#   `Dew point temperature(°C)` &lt;dbl&gt;, `Solar Radiation (MJ/m2)` &lt;dbl&gt;,\n#   `Rainfall(mm)` &lt;dbl&gt;, `Snowfall (cm)` &lt;dbl&gt;, date &lt;date&gt;, seasons &lt;fct&gt;,\n#   holiday &lt;fct&gt;, fn_day &lt;fct&gt;\n\n\n\n# Lastly, rename the all the variables to have easy to use names (I use lower snake case but whatever you’d like is fine\nbike_data &lt;- bike_data |&gt;\nrename('bike_count' = `Rented Bike Count`,\n'hour' = \"Hour\",\n\"temp\" = `Temperature(°C)`,\n\"wind_speed\" = `Wind speed (m/s)`,\n\"humidity\" = `Humidity(%)`,\n\"vis\" = `Visibility (10m)`,\n\"dew_point_temp\" = `Dew point temperature(°C)`,\n\"solar_radiation\" = `Solar Radiation (MJ/m2)`,\n\"rainfall\" = \"Rainfall(mm)\",\n\"snowfall\" = `Snowfall (cm)`)\nbike_data\n\n# A tibble: 8,760 × 14\n   bike_count  hour  temp humidity wind_speed   vis dew_point_temp\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1        254     0  -5.2       37        2.2  2000          -17.6\n 2        204     1  -5.5       38        0.8  2000          -17.6\n 3        173     2  -6         39        1    2000          -17.7\n 4        107     3  -6.2       40        0.9  2000          -17.6\n 5         78     4  -6         36        2.3  2000          -18.6\n 6        100     5  -6.4       37        1.5  2000          -18.7\n 7        181     6  -6.6       35        1.3  2000          -19.5\n 8        460     7  -7.4       38        0.9  2000          -19.3\n 9        930     8  -7.6       37        1.1  2000          -19.8\n10        490     9  -6.5       27        0.5  1928          -22.4\n# ℹ 8,750 more rows\n# ℹ 7 more variables: solar_radiation &lt;dbl&gt;, rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;,\n#   date &lt;date&gt;, seasons &lt;fct&gt;, holiday &lt;fct&gt;, fn_day &lt;fct&gt;\n\n\n\n# fn_day variable implies they were out of commission sometimes. Let’s remove those observations and that variable\nbike_data &lt;- bike_data |&gt;\nfilter(fn_day == \"Yes\") |&gt;\nselect(-fn_day)\nbike_data\n\n# A tibble: 8,465 × 13\n   bike_count  hour  temp humidity wind_speed   vis dew_point_temp\n        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1        254     0  -5.2       37        2.2  2000          -17.6\n 2        204     1  -5.5       38        0.8  2000          -17.6\n 3        173     2  -6         39        1    2000          -17.7\n 4        107     3  -6.2       40        0.9  2000          -17.6\n 5         78     4  -6         36        2.3  2000          -18.6\n 6        100     5  -6.4       37        1.5  2000          -18.7\n 7        181     6  -6.6       35        1.3  2000          -19.5\n 8        460     7  -7.4       38        0.9  2000          -19.3\n 9        930     8  -7.6       37        1.1  2000          -19.8\n10        490     9  -6.5       27        0.5  1928          -22.4\n# ℹ 8,455 more rows\n# ℹ 6 more variables: solar_radiation &lt;dbl&gt;, rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;,\n#   date &lt;date&gt;, seasons &lt;fct&gt;, holiday &lt;fct&gt;\n\n\n\n# To simplify our analysis, we’ll summarize across the hours so that each day has one observation associated with it. Let’s group_by() the date, seasons, and holiday variables and find the sum of the bike_count, rainfall, and snowfall variables and the mean of all the weather related variables.\nbike_data &lt;- bike_data |&gt;\n  group_by(date, seasons, holiday) |&gt;\n  summarize(bike_count = sum(bike_count),\n      temp = mean(temp),\n      humidity = mean(humidity),\n      wind_speed = mean(wind_speed),\n      vis = mean(vis),\n      dew_point_temp = mean(dew_point_temp),\n      solar_radiation = mean(solar_radiation),\n      rainfall = sum(rainfall),\n      snowfall = sum(snowfall)) |&gt;\nungroup()\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\nbike_data\n\n# A tibble: 353 × 12\n   date       seasons holiday    bike_count    temp humidity wind_speed   vis\n   &lt;date&gt;     &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1 2017-12-01 Winter  No Holiday       9539 -2.45       45.9      1.54  1871.\n 2 2017-12-02 Winter  No Holiday       8523  1.32       62.0      1.71  1471.\n 3 2017-12-03 Winter  No Holiday       7222  4.88       81.5      1.61   456.\n 4 2017-12-04 Winter  No Holiday       8729 -0.304      52.5      3.45  1363.\n 5 2017-12-05 Winter  No Holiday       8307 -4.46       36.4      1.11  1959.\n 6 2017-12-06 Winter  No Holiday       6669  0.0458     70.8      0.696 1187.\n 7 2017-12-07 Winter  No Holiday       8549  1.09       67.5      1.69   949.\n 8 2017-12-08 Winter  No Holiday       8032 -3.82       41.8      1.85  1872.\n 9 2017-12-09 Winter  No Holiday       7233 -0.846      46        1.08  1861.\n10 2017-12-10 Winter  No Holiday       3453  1.19       69.7      2.00  1043.\n# ℹ 343 more rows\n# ℹ 4 more variables: dew_point_temp &lt;dbl&gt;, solar_radiation &lt;dbl&gt;,\n#   rainfall &lt;dbl&gt;, snowfall &lt;dbl&gt;\n\n\n\n# Recreate your basic summary stats and then create some plots to explore relationships. Report correlation between your numeric variables as well\n# Looks to be right skewed with a pretty large standard deviation.\nbike_data |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 1 × 6\n  bike_count_mean bike_count_median bike_count_sd bike_count_IQR bike_count_min\n            &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1          17485.             18563         9937.          19318            977\n# ℹ 1 more variable: bike_count_max &lt;dbl&gt;\n\n\n\nbike_data |&gt;\ngroup_by(holiday) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 2 × 7\n  holiday    bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;                &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Holiday             12700.             7184         10504.         16576 \n2 No Holiday          17727.            19104.         9862.         19168.\n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\n\nbike_data |&gt;\ngroup_by(seasons) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n# A tibble: 4 × 7\n  seasons bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Autumn           22099.            23350          6711.         10733 \n2 Spring           17910.            17590          8357.         14362.\n3 Summer           24818.            25572.         7297.          9308.\n4 Winter            5413.             5498          1808.          2634.\n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\n\n# Differences are pretty big in the Winter and Autumn but not the other seasons. Perhaps an interaction between these two variables is important.\nbike_data |&gt;\ngroup_by(seasons, holiday) |&gt;\nsummarize(across(`bike_count`,\n.fns = c(\"mean\" = mean,\n\"median\" = median,\n\"sd\" = sd,\n\"IQR\" = IQR,\n\"min\" = min,\n\"max\" = max),\n.names = \"{.col}_{.fn}\"))\n\n`summarise()` has grouped output by 'seasons'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 8\n# Groups:   seasons [4]\n  seasons holiday bike_count_mean bike_count_median bike_count_sd bike_count_IQR\n  &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1 Autumn  Holiday          22754.            21705          5642.          5740 \n2 Autumn  No Hol…          22065.            23472          6792.         10734 \n3 Spring  Holiday          15247.            13790         10917.         10844 \n4 Spring  No Hol…          18002.            17730          8322.         14224.\n5 Summer  Holiday          24532.            24532.         8438.          5966.\n6 Summer  No Hol…          24824.            25572.         7324.          9165 \n7 Winter  Holiday           3759              3454.         1561.          1060.\n8 Winter  No Hol…           5574.             5609          1757.          2564 \n# ℹ 2 more variables: bike_count_min &lt;dbl&gt;, bike_count_max &lt;dbl&gt;\n\n\n\n# do some correlation\n# Definitely a few moderate relationships with bike_count here (temp and solar_radiation). temp and dew_point_temp are obviously pretty related. humidity and vis along with humidity and dew_point_temp as well.\nbike_data |&gt;\nselect(where(is.numeric)) |&gt;\ncor() |&gt;\nround(3)\n\n                bike_count   temp humidity wind_speed    vis dew_point_temp\nbike_count           1.000  0.753    0.036     -0.193  0.166          0.650\ntemp                 0.753  1.000    0.404     -0.261  0.002          0.963\nhumidity             0.036  0.404    1.000     -0.234 -0.559          0.632\nwind_speed          -0.193 -0.261   -0.234      1.000  0.206         -0.288\nvis                  0.166  0.002   -0.559      0.206  1.000         -0.154\ndew_point_temp       0.650  0.963    0.632     -0.288 -0.154          1.000\nsolar_radiation      0.736  0.550   -0.274      0.096  0.271          0.383\nrainfall            -0.239  0.145    0.529     -0.102 -0.222          0.265\nsnowfall            -0.265 -0.267    0.065      0.021 -0.102         -0.210\n                solar_radiation rainfall snowfall\nbike_count                0.736   -0.239   -0.265\ntemp                      0.550    0.145   -0.267\nhumidity                 -0.274    0.529    0.065\nwind_speed                0.096   -0.102    0.021\nvis                       0.271   -0.222   -0.102\ndew_point_temp            0.383    0.265   -0.210\nsolar_radiation           1.000   -0.323   -0.233\nrainfall                 -0.323    1.000   -0.023\nsnowfall                 -0.233   -0.023    1.000\n\n\n\n# do some visualizations. Some expected trends here and we can see that once it gets pretty hot, bike rentals slow.\nggplot(bike_data, aes(x = temp, y = bike_count)) +\ngeom_jitter(aes(color = seasons)) +\nfacet_grid(~holiday)\n\n\n\n\n\n# More solar radiation is associated with more bike rentals\nggplot(bike_data, aes(x = solar_radiation, y = bike_count)) +\ngeom_point(aes(color = seasons)) +\nfacet_grid(~holiday)\n\n\n\n\n\n# Use functions from tidymodels to split the data into a training and test set (75/25 split). Use the strata argument to stratify the split on the seasons variable. On the training set, create a 10 fold CV split\nset.seed(11)\nbike_split &lt;- initial_split(bike_data, prop = 0.75, strata = seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\nbike_10_fold &lt;- vfold_cv(bike_train, 10)\n\nbike_10_fold\n\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [236/27]&gt; Fold01\n 2 &lt;split [236/27]&gt; Fold02\n 3 &lt;split [236/27]&gt; Fold03\n 4 &lt;split [237/26]&gt; Fold04\n 5 &lt;split [237/26]&gt; Fold05\n 6 &lt;split [237/26]&gt; Fold06\n 7 &lt;split [237/26]&gt; Fold07\n 8 &lt;split [237/26]&gt; Fold08\n 9 &lt;split [237/26]&gt; Fold09\n10 &lt;split [237/26]&gt; Fold10\n\n\n\n# create some recipes\n\nMLR_rec1 &lt;- recipe(bike_count ~ ., data = bike_train) |&gt;\n  step_date(date, features = \"dow\") |&gt;\n  step_mutate(day_type = factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))) |&gt;\n  step_rm(date, date_dow) |&gt;\n  step_dummy(seasons, holiday, day_type) |&gt;\n  step_normalize(all_numeric(), -bike_count)  \nMLR_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\"))\n\n\n• Variables removed: date and date_dow\n\n\n• Dummy variables from: seasons, holiday, day_type\n\n\n• Centering and scaling for: all_numeric() and -bike_count\n\n\n\n# For the 2nd recipe\nMLR_rec2 &lt;- MLR_rec1 |&gt;\nstep_interact(terms = ~starts_with(\"seasons\")*starts_with(\"holiday\") +\nstarts_with(\"seasons\")*temp +\ntemp*rainfall)\nMLR_rec2\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Operations \n\n\n• Date features from: date\n\n\n• Variable mutation for: factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\"))\n\n\n• Variables removed: date and date_dow\n\n\n• Dummy variables from: seasons, holiday, day_type\n\n\n• Centering and scaling for: all_numeric() and -bike_count\n\n\n• Interactions with: ...\n\n\n\n# For the 3rd recipe\nMLR_rec3 &lt;- MLR_rec2 |&gt;\n  step_poly(temp,\n    wind_speed,\n    vis,\n    dew_point_temp,\n    solar_radiation,\n    rainfall,\n    snowfall,\n    degree = 2)\n\n\n# Now we can set up our linear model fit\nMLR_spec &lt;- linear_reg() |&gt;\nset_engine(\"lm\")\n\n\nMLR_CV_fit1 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec1) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n\nMLR_CV_fit2 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec2) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n\nMLR_CV_fit3 &lt;- workflow() |&gt;\nadd_recipe(MLR_rec3) |&gt;\nadd_model(MLR_spec) |&gt;\nfit_resamples(bike_10_fold)\n  \n    \nMLR_CV_fit1\n\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits           id     .metrics         .notes          \n   &lt;list&gt;           &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;          \n 1 &lt;split [236/27]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 2 &lt;split [236/27]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 3 &lt;split [236/27]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 4 &lt;split [237/26]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 5 &lt;split [237/26]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 6 &lt;split [237/26]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 7 &lt;split [237/26]&gt; Fold07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 8 &lt;split [237/26]&gt; Fold08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n 9 &lt;split [237/26]&gt; Fold09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n10 &lt;split [237/26]&gt; Fold10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\n# Get our metrics. The last model appears to be the best\nrbind(MLR_CV_fit1 |&gt; collect_metrics(),\nMLR_CV_fit2 |&gt; collect_metrics(),\nMLR_CV_fit3 |&gt; collect_metrics())\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4284.       10 165.     Preprocessor1_Model1\n2 rsq     standard      0.822    10   0.0151 Preprocessor1_Model1\n3 rmse    standard   3156.       10 267.     Preprocessor1_Model1\n4 rsq     standard      0.898    10   0.0176 Preprocessor1_Model1\n5 rmse    standard   3070.       10 213.     Preprocessor1_Model1\n6 rsq     standard      0.903    10   0.0142 Preprocessor1_Model1\n\n\n\n# fit that to the entire training set and then see how it performs on the test set.\nfinal_fit &lt;- workflow() |&gt;\nadd_recipe(MLR_rec1) |&gt;\nadd_model(MLR_spec) |&gt;\nlast_fit(bike_split)\n\nfinal_fit |&gt;\ncollect_metrics() |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3980.    Preprocessor1_Model1\n2 rsq     standard       0.846 Preprocessor1_Model1\n\n\n\n# Obtain the final model (fit on the entire training set) coefficient table using tidy().\nfinal_fit |&gt;\nextract_fit_parsnip() |&gt;\ntidy()|&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3)))\n\n# A tibble: 14 × 5\n   term               estimate std.error statistic p.value\n   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)         17446.       252.    69.3     0    \n 2 temp                -2439.      5215.    -0.468   0.64 \n 3 humidity            -1927.      1904.    -1.01    0.313\n 4 wind_speed           -523.       286.    -1.83    0.069\n 5 vis                   -63.7      361.    -0.177   0.86 \n 6 dew_point_temp       7143.      6143.     1.16    0.246\n 7 solar_radiation      4088.       473.     8.64    0    \n 8 rainfall            -1779.       333.    -5.35    0    \n 9 snowfall             -317.       276.    -1.15    0.25 \n10 seasons_Spring      -2528.       355.    -7.12    0    \n11 seasons_Summer      -1670.       442.    -3.78    0    \n12 seasons_Winter      -3684.       501.    -7.35    0    \n13 holiday_No.Holiday    835.       256.     3.26    0.001\n14 day_type_Weekend    -1050.       256.    -4.10    0    \n\n\n\n# My homework 9 starts here...\n# define LASSO model specification\nlasso_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt;\n  set_engine(\"glmnet\")\n# define recipe. Use the same preprocessing steps as the previous MLR model (MLR_rec1)\nlasso_rec &lt;- MLR_rec1\n\nlasso_workflow &lt;- workflow() |&gt;\n  add_recipe(lasso_rec) |&gt;\n  add_model(lasso_spec)\n\n# define cross-validation and tuning grid\nset.seed(123)\nbike_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Define a grid of penalty values\npenalty_grid &lt;- grid_regular(penalty(range = c(-3, 1)), levels = 30) # log10 scale\n\n# tuning\nlasso_tune &lt;- lasso_workflow |&gt;\n  tune_grid(\n    resamples = bike_folds,\n    grid = penalty_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nWarning: package 'Matrix' was built under R version 4.3.3\n\n# per Dr. Post, use select_best() and use the smallest penalty\nlasso_best &lt;- lasso_tune |&gt;\n  select_best(metric = \"rmse\")\nlasso_best\n\n# A tibble: 1 × 2\n  penalty .config              \n    &lt;dbl&gt; &lt;chr&gt;                \n1   0.001 Preprocessor1_Model01\n\nlasso_final_workflow &lt;- lasso_workflow |&gt;\n  finalize_workflow(lasso_best)\n\nlasso_final_fit &lt;- lasso_final_workflow |&gt;\n  last_fit(bike_split)\nlasso_metrics &lt;- lasso_final_fit |&gt;\n  collect_metrics()\n\n# Display metrics. It shows rmse = 3999.44, rsq = 0.844\nlasso_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3999.    Preprocessor1_Model1\n2 rsq     standard       0.844 Preprocessor1_Model1\n\n# or display them all. However, LASSO does not seem to be a good fit.\nrbind(\n  lasso_tune |&gt; collect_metrics() |&gt; mutate(model = \"LASSO\")\n)\n\n# A tibble: 60 × 8\n   penalty .metric .estimator     mean     n  std_err .config              model\n     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;\n 1 0.001   rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 2 0.001   rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 3 0.00137 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 4 0.00137 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 5 0.00189 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 6 0.00189 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 7 0.00259 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n 8 0.00259 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n 9 0.00356 rmse    standard   4249.       10 147.     Preprocessor1_Model… LASSO\n10 0.00356 rsq     standard      0.816    10   0.0118 Preprocessor1_Model… LASSO\n# ℹ 50 more rows\n\n\n\n# add a (tuned) regression Tree model\n# define regression tree model specification\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(), \n  tree_depth = tune(),      \n  min_n = tune()            \n) |&gt; \n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# recipe. Use the same recipe as before (MLR_rec1).\ntree_rec &lt;- MLR_rec1\n# Combine the recipe and the model specification.\ntree_workflow &lt;- workflow() |&gt;\n  add_recipe(tree_rec) |&gt;\n  add_model(tree_spec)\n\nset.seed(123)\nbike_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Define a tuning grid\ntree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-3, 0)), # Log10 scale\n  tree_depth(range = c(1, 10)),\n  min_n(range = c(2, 10)),\n  levels = 5\n)\n\n# Below: issue with some computations. estimate is constant and has 0 standard deviation (folder might have insufficient variability in 'bike_count')\ntree_tune &lt;- tree_workflow |&gt;\n  tune_grid(\n    resamples = bike_folds,\n    grid = tree_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\ntree_best &lt;- tree_tune |&gt;\n  select_best(metric = \"rmse\")\n\n# Print the best hyperparameters. Return: cost_complexity 0.001, tree_depth 7, and min_n 8.\ntree_best\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config               \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                 \n1           0.001          7     8 Preprocessor1_Model091\n\n# use the best hyperparameters to finalize workflow\ntree_final_workflow &lt;- tree_workflow |&gt;\n  finalize_workflow(tree_best)\n\n# fit the best regression tree model on entire training data\ntree_final_fit &lt;- tree_final_workflow |&gt;\n  last_fit(bike_split)\n\ntree_metrics &lt;- tree_final_fit |&gt;\n  collect_metrics()\n\n# Display metrics. rmse = 3046.078, rsq = 0.91\ntree_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3046.    Preprocessor1_Model1\n2 rsq     standard       0.911 Preprocessor1_Model1\n\n\n\n# Add a (tuned) bagged tree model\n# Define the Bagged Tree Model Specification. Bagging: creating multiple decision trees and averaging their predictions\nlibrary(baguette)\n\nWarning: package 'baguette' was built under R version 4.3.3\n\n# \n# bagged_tree_spec &lt;- bag_tree() |&gt;\n#   set_engine(\"rpart\", times = tune()) |&gt;\n#   set_mode(\"regression\")\n\n\nbagged_tree_spec &lt;- bag_tree(\n  cost_complexity = tune(),  # Complexity parameter for pruning\n  tree_depth = tune(),       # Maximum depth of individual trees\n  min_n = tune()             # Minimum number of observations in a terminal node\n) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n# Use the same preprocessing recipe (MLR_rec1)\n\nbagged_tree_rec &lt;- MLR_rec1\n\n# Create the Workflow, Combine the recipe and model specification into a workflow\nbagged_tree_workflow &lt;- workflow() |&gt;\n  add_recipe(bagged_tree_rec) |&gt;\n  add_model(bagged_tree_spec)\n\n# Define Cross-Validation and Tuning Grid\n\nbagged_tree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-3, 0)), # Log10 scale\n  tree_depth(range = c(5, 20)),     # Depth of the tree\n  min_n(range = c(2, 10)),          # Minimum samples per node\n  levels = 5\n)\n\n# # following is commanded out for now.\n# \n# set.seed(123)\n# bagged_tree_folds &lt;- vfold_cv(bike_train, v = 10)\n# \n# # Define the tuning grid\n# bagged_tree_grid &lt;- grid_regular(\n#   times(range = c(10, 50)), # Number of trees to bag\n#   levels = 5\n# )\n# \n# bagged_tree_grid &lt;- tibble(\n#   times = seq(10, 50, by = 10) # Specify the number of trees to bag\n# )\n\n# perform tuning. error\nset.seed(123)\n\nbagged_tree_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# long execution time\nbagged_tree_tune &lt;- bagged_tree_workflow |&gt;\n  tune_grid(\n    resamples = bagged_tree_folds,\n    grid = bagged_tree_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\nThere were issues with some computations   A: x6\n\n\nThere were issues with some computations   A: x7\n\n\nThere were issues with some computations   A: x8\n\n\nThere were issues with some computations   A: x9\n\n\nThere were issues with some computations   A: x10\nThere were issues with some computations   A: x10\n\n\n\n\nbagged_tree_metrics &lt;- bagged_tree_tune |&gt; \n  collect_metrics() |&gt; \n  mutate(model = \"Bagged Tree\")\n\nrbind(bagged_tree_metrics)\n\n# A tibble: 250 × 10\n   cost_complexity tree_depth min_n .metric .estimator     mean     n  std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n 1         0.001            5     2 rmse    standard   3369.       10 220.    \n 2         0.001            5     2 rsq     standard      0.885    10   0.0144\n 3         0.00562          5     2 rmse    standard   3509.       10 173.    \n 4         0.00562          5     2 rsq     standard      0.877    10   0.0145\n 5         0.0316           5     2 rmse    standard   4278.       10 224.    \n 6         0.0316           5     2 rsq     standard      0.812    10   0.0272\n 7         0.178            5     2 rmse    standard   5773.       10 445.    \n 8         0.178            5     2 rsq     standard      0.661    10   0.0515\n 9         1                5     2 rmse    standard   9878.       10 151.    \n10         1                5     2 rsq     standard    NaN         0  NA     \n# ℹ 240 more rows\n# ℹ 2 more variables: .config &lt;chr&gt;, model &lt;chr&gt;\n\n\n\nrf_spec &lt;- rand_forest(\n  mtry = tune(),   # Number of predictors to randomly select at each split\n  trees = tune(),  # Number of trees in the forest\n  min_n = tune()   # Minimum number of data points in a terminal node\n) |&gt;\n  set_engine(\"randomForest\") |&gt;  # Use the 'randomForest' engine\n  set_mode(\"regression\")\n\n# Use the same preprocessing recipe (MLR_rec1)\nrf_rec &lt;- MLR_rec1\n\n# Combine the recipe and the Random Forest model into a workflow\nrf_workflow &lt;- workflow() |&gt;\n  add_recipe(rf_rec) |&gt;\n  add_model(rf_spec)\n\n\nset.seed(123)\nrf_folds &lt;- vfold_cv(bike_train, v = 10)\n\n# Define a tuning grid for the hyperparameters\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, ncol(bike_train) - 1)), # Number of predictors\n  trees(range = c(50, 500)),               # Number of trees\n  min_n(range = c(5, 20)),                 # Minimum node size\n  levels = 5                               # Levels for each parameter\n)\n\n\n# perform tuning. run the tuning process\nset.seed(123)\nrf_tune &lt;- rf_workflow |&gt;\n  tune_grid(\n    resamples = rf_folds,\n    grid = rf_grid,\n    metrics = metric_set(rmse, rsq)\n  )\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\nrf_metrics &lt;- rf_tune |&gt; \n  collect_metrics() |&gt; \n  mutate(model = \"Random Forest\")\n\nrbind(rf_metrics)\n\n# A tibble: 250 × 10\n    mtry trees min_n .metric .estimator     mean     n   std_err .config   model\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;\n 1     2    50     5 rmse    standard   3453.       10 179.      Preproce… Rand…\n 2     2    50     5 rsq     standard      0.891    10   0.00907 Preproce… Rand…\n 3     4    50     5 rmse    standard   3167.       10 160.      Preproce… Rand…\n 4     4    50     5 rsq     standard      0.903    10   0.00921 Preproce… Rand…\n 5     6    50     5 rmse    standard   3087.       10 185.      Preproce… Rand…\n 6     6    50     5 rsq     standard      0.907    10   0.0119  Preproce… Rand…\n 7     8    50     5 rmse    standard   3054.       10 205.      Preproce… Rand…\n 8     8    50     5 rsq     standard      0.907    10   0.0131  Preproce… Rand…\n 9    11    50     5 rmse    standard   3094.       10 199.      Preproce… Rand…\n10    11    50     5 rsq     standard      0.905    10   0.0121  Preproce… Rand…\n# ℹ 240 more rows\n\n# sort by rmse\nrf_best_rmse &lt;- rf_metrics |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  arrange(mean)\n\n# by r2\nrf_best_rsq &lt;- rf_metrics |&gt;\n  filter(.metric == \"rsq\") |&gt;\n  arrange(desc(mean))\n\n\n# finalize and evaluate it on the test set\nrf_best &lt;- rf_tune |&gt; select_best(metric = \"rmse\")\n\n\nfinal_rf_workflow &lt;- rf_workflow |&gt; \n  finalize_workflow(rf_best)\n\nfinal_rf_fit &lt;- final_rf_workflow |&gt; \n  last_fit(bike_split)\n\n# Collect metrics from the test set\nfinal_rf_metrics &lt;- final_rf_fit |&gt; \n  collect_metrics()\n\nfinal_rf_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    2670.    Preprocessor1_Model1\n2 rsq     standard       0.932 Preprocessor1_Model1\n\n\n\nlasso_best &lt;- lasso_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_lasso_workflow &lt;- lasso_workflow |&gt; \n  finalize_workflow(lasso_best)\n\n# Save test set evaluation for LASSO\nlasso_metrics &lt;- final_lasso_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"LASSO\")\n\n\n# regression tree\nreg_tree_best &lt;- tree_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_regression_tree_workflow &lt;- tree_final_workflow |&gt; \n  finalize_workflow(reg_tree_best)\n\nreg_tree_metrics &lt;- tree_final_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt; \n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt; \n  mutate(model = \"Regression Tree\")\n\n# bagged tree\nbagged_tree_best &lt;- bagged_tree_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_bagged_tree_workflow &lt;- bagged_tree_workflow |&gt; \n  finalize_workflow(bagged_tree_best)\n\n# Save test set evaluation for Bagged Tree\nbagged_tree_metrics &lt;- final_bagged_tree_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"Bagged Tree\")\n\n\n# random forest model\nrf_best &lt;- rf_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_rf_workflow &lt;- rf_workflow |&gt; \n  finalize_workflow(rf_best)\n\n# Save test set evaluation for Random Forest\nrf_metrics &lt;- final_rf_workflow |&gt;\n  last_fit(bike_split) |&gt;\n  collect_metrics() |&gt;\n  filter(.metric %in% c(\"rmse\", \"mae\")) |&gt;\n  mutate(model = \"Random Forest\")\n\n\nall_metrics &lt;- bind_rows(\n  lasso_metrics,\n  reg_tree_metrics,\n  bagged_tree_metrics,\n  rf_metrics\n)\n\nprint(all_metrics)\n\n# A tibble: 4 × 5\n  .metric .estimator .estimate .config              model          \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;          \n1 rmse    standard       3999. Preprocessor1_Model1 LASSO          \n2 rmse    standard       3046. Preprocessor1_Model1 Regression Tree\n3 rmse    standard       2954. Preprocessor1_Model1 Bagged Tree    \n4 rmse    standard       2750. Preprocessor1_Model1 Random Forest  \n\n\n\nlasso_best &lt;- lasso_tune |&gt; select_best(metric = \"rmse\")\n\nfinal_lasso_workflow &lt;- lasso_workflow |&gt; \n  finalize_workflow(lasso_best)\n\nfinal_lasso_fit &lt;- final_lasso_workflow |&gt; \n  fit(data = bike_train)\n\nlasso_model &lt;- final_lasso_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\nprint(lasso_model)\n\n# A tibble: 14 × 3\n   term               estimate penalty\n   &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)          17446.   0.001\n 2 temp                   389.   0.001\n 3 humidity              -887.   0.001\n 4 wind_speed            -522.   0.001\n 5 vis                      0    0.001\n 6 dew_point_temp        3752.   0.001\n 7 solar_radiation       4065.   0.001\n 8 rainfall             -1841.   0.001\n 9 snowfall              -336.   0.001\n10 seasons_Spring       -2505.   0.001\n11 seasons_Summer       -1607.   0.001\n12 seasons_Winter       -3653.   0.001\n13 holiday_No.Holiday     820.   0.001\n14 day_type_Weekend     -1060.   0.001\n\n\n\nfinal_fit &lt;- workflow |&gt; \n  fit(data = bike_train)\n\nmlr_model &lt;- final_mlr_fit |&gt; \n  extract_fit_parsnip() |&gt; \n  tidy()\n\nprint(mlr_model)\n\n\nreg_tree_model &lt;- tree_final_fit |&gt; \n  extract_fit_engine()\n\nif (!requireNamespace(\"rpart.plot\", quietly = TRUE)) {\n  install.packages(\"rpart.plot\")\n}\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.3.3\n\n# Plot the regression tree\nrpart.plot(reg_tree_model)\n\nWarning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\nTo silence this warning:\n    Call rpart.plot with roundint=FALSE,\n    or rebuild the rpart model with model=TRUE.\n\n\n\n\n\n\nfinal_bagged_tree_fit &lt;- final_bagged_tree_workflow |&gt; \n  fit(data = bike_train)\n\nfinal_rf_fit &lt;- final_rf_workflow |&gt; \n  fit(data = bike_train)\n\nrf_model &lt;- final_rf_fit |&gt; \n  extract_fit_engine()\n\n# # Extract variable importance\n# rf_importance &lt;- as.data.frame(rf_model$variable.importance) |&gt; \n#   rownames_to_column(var = \"Variable\") |&gt; \n#   rename(Importance = 2) |&gt; \n#   arrange(desc(Importance))\n\nif (!is.null(rf_model$importance)) {\n  rf_importance &lt;- as.data.frame(rf_model$importance) |&gt;\n    tibble::rownames_to_column(var = \"Variable\") |&gt;\n    arrange(desc(IncNodePurity))\n} else {\n  stop(\"Variable importance is not available in the model output.\")\n}\n\nlibrary(ggplot2)\n\nrf_importance &lt;- as.data.frame(rf_model$importance, stringsAsFactors = FALSE) |&gt; \n  tibble::rownames_to_column(var = \"Variable\") |&gt; \n  rename(Importance = IncNodePurity)\n\nprint(rf_importance)\n\n             Variable  Importance\n1                temp 12438261261\n2            humidity   744875642\n3          wind_speed   354338807\n4                 vis   506832939\n5      dew_point_temp  2391134881\n6     solar_radiation  4674690597\n7            rainfall  1060319136\n8            snowfall    20765096\n9      seasons_Spring   256379773\n10     seasons_Summer   115005444\n11     seasons_Winter  2659083462\n12 holiday_No.Holiday    58525762\n13   day_type_Weekend   109391163\n\nggplot(rf_importance, aes(x = reorder(Variable, Importance), y = Importance)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance - Random Forest\",\n    x = \"Variable\",\n    y = \"Importance\"\n  )"
  }
]